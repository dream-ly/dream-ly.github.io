<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.7/css/all.min.css">
    <base href="https://www.sfu.ca/~yla879/">
    <!-- <base href="D:/KeyFile/Self-presentation/Personal Website/~yla879/"> -->
    <title>Ying Lei | HCI researcher</title>
    <style>
        body {
            margin-top: 20px;
            margin-bottom: 30px;
            font-family: sans-serif;
            font-weight: lighter;
        }

        a {
            color: black;
            border-bottom: 1px dotted black;
        }

        a:hover,
        a:active {
            color: #DB522F;
            text-decoration: none;
        }

        h1 a {
            color: #6B747C;
            border: none;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 2px solid;
        }

        h3 {
            font-size: 1.2em;
            color: #000000;
        }

        h4 {
            font-size: 1em;
            font-family: sans-serif;
            font-weight: lighter;
            color: #000000;
            margin-top: 10px;
            margin-bottom: 30px;
        }

        .strong {
            color: #DB522F;
        }

        @media (max-width: 767.98px) {
            header {
                text-align: center;
            }
        }

        ul.social-icons {
            font-size: 1rem;
            margin-top: 24px;
            margin-bottom: 0;
        }

        ul.social-icons li::before {
            content: '[';
        }

        ul.social-icons li::after {
            content: ']';
        }

        ul.social-icons li a {
            border: none;
        }

        img.portrait {
            max-width: 100%;
        }

        @media (max-width: 767.98px) {
            img.portrait {
                display: block;
                max-width: 300px;
                margin: auto;
            }
        }

        .annotation {
            margin-top: -0.5em;
            margin-bottom: 0.5em;
            font-size: 12px;
            line-height: 12px;
        }

        .taxonomy img {
            max-width: 100%;
        }

        div.research-project {
            font-size: 14px;
            margin-bottom: 1.5rem;
        }

        div.line-of-research {
            background-color: #F0F0F0;
        }

        div.research-project video {
            max-width: 100%;
            margin-bottom: 0.5rem;
        }

        div.research-project p {
            margin-bottom: 0.3rem;
        }

        .news {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #news-more {
            display: none;
        }

        .press {
            font-size: 15px;
            margin-bottom: 0px;
        }
        
        .award {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #award-more {
            display: none;
        }

        .tweets {
            overflow: auto;
            -webkit-overflow-scrolling: touch;
        }

        .info {
            border-style: none;
            font-weight: bold;
            color: #999;
        }

        hr.dash {
            border-top: 1px dashed #bbbbbb;
            margin-bottom: 15px;
            margin-top: 15px;
        }

        .switch {
            position: relative;
            display: block;
            width: 32px;
            height: 18px;
            float: left;
			top: 3px;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 3px;
            right: 0;
            bottom: -3px;
            left: 0;
            background-color: #ccc;
            transition: 0.5s;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 12px;
            width: 12px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: 0.5s;
        }

        .active .slider {
            background-color: #DB522F;
        }

        .active .slider:before {
            transform: translateX(14px);
        }
		
		.slider.round {
		  border-radius: 34px;
		}

		.slider.round:before {
		  border-radius: 50%;
		}
    </style>
	
	<!-- Google Tag Manager -->
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-PFGW5C3');</script>
	<!-- End Google Tag Manager -->


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-48610112-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-48610112-3');
    </script>

</head>

<body>
	
	<!-- Google Tag Manager (noscript) -->
	<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFGW5C3"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->
	
    <header class="container">
        <h1 class="float-md-left mb-0">
            <a href="/">Ying Lei</a>
        </h1>
        <ul class="list-inline float-md-right social-icons">
            <!-- <li class="list-inline-item"><a href="/~yla879/Resume_Ying_Lei.pdf">CV/Resume</a></li> -->
            <!-- <li class="list-inline-item"><a href="/~yla879/Portfolio_Ying_Lei.pdf">Portfolio</a></li> -->
            <li class="list-inline-item"><a href="https://scholar.google.com.hk/citations?user=AuxxOSYAAAAJ&hl">Google Scholar</a></li>
            <!-- <li class="list-inline-item"><a href="https://github.com/dream-ly">Github</a></li> -->
            <li class="list-inline-item"><a href="https://www.linkedin.com/in/ying-lei-3a8a6035b/">Linkedin</a></li>
            <li class="list-inline-item"><a href="https://x.com/yinghci">Twitter</a></li>
            <li class="list-inline-item"><a class="email-anchor">Email</a></li>
        </ul>
        <div class="clearfix"></div>
        <hr>
    </header>

    <div class="container">
        <div class="row mb-3">
            <div class="col-xl-8 col-lg-9 col-md-8">
                <p>
                    Ying Lei is a researcher working at the intersection of Human-Computer Interaction, Human-centered AI, CSCW, and Interaction Design.
                    She adopts theory-driven (from sociology, psychology, neuroscience), empirical approaches (both qualitative and quantitative) to generate innovations in interaction.
                </p>
                <p>
                    Ying is passion about perceiving, modeling, and augument human behaviors and cognition in various Human-AI Interaction scenarios, 
                    such as GenAI-enabled Digital Immortality, AI-assisted decision-making, and behavior intervention.
                    She is also interested in understanding and designing for the social connections and support of underrepresented groups, such as late-life migrants and blended families.
                    Grounded in socio-technical perspectives, her work aims to enhance individual well-being and real-world impact 
                    through the human-centered design of emerging technologies, with social, emotional, and ethical considerations.
                </p>
                <p>
                    Ying has multidisciplinary training in technology and design.
                    She is now pursuing her MSc, advised by <a href="https://carmann.ca/">Prof. Carman Neustaedter</a> at the
                    <a href="https://www.sfu.ca/siat.html">School of Interactive Art and Technology</a>, 
                    <a href="https://www.sfu.ca/">Simon Fraser University</a>. 
                    Previously, she earned B.Eng. in Computer Science, <a href="https://english.ecnu.edu.cn/">East China Normal University</a> 
                    with <a href="https://cs.ecnu.edu.cn/b6/66/c19867a505446/page.htm">the highest honors</a>.
                </p>
				
                <p>
                    Her work mainly publishes at <a href="https://sigchi.org/">SIGCHI</a> and has attracted attention from academic and public audiences. 
                    Ying has received scholarships and fellowships from multiple institutions and corporations, 
                    such as
                    <a href="https://www.sfu.ca/gradstudies/awards-funding.html">SFU</a>, 
                    <a href="https://www.sfu.ca/fcat/current-students/financial-awards.html">SFU FCAT</a>,
                    <a href="http://gjcxcy.bjtu.edu.cn/NewJTItemListForStudentDetail.aspx?ItemNo=925997&year=2023&type=student&IsLXItem=0">IETP</a>, and
                    <a href="https://cs-cxcy.ecnu.edu.cn/cd/8b/c34649a445835/page.htm">Huaxin</a>.
                </p>                
				
					
            </div>
            <div class="offset-xl-1 col-lg-3 col-md-4">
                <img src="/~yla879/image/me_portrait_medium.jpg" class="portrait" alt="a portrait of ying lei">
            </div>
        </div>

        <!-- <div class="row taxonomy">
            <div class="col-lg-6 mb-3">
                <img src="/~yla879/image/Taxonomy_YZ1.png" alt="a Venn diagram of my research focus, which includes interactivity and activity recognition">
            </div>
            <div class="col-lg-6 mb-3">
                <img src="/~yla879/image/Taxonomy_YZ2.png" alt="a texonomy of my research methodology which includes enhanced sensing for mobile devices, sensing through everyday objects, and wide-area sensing for smart environments, all of which center around the topic of sensing technology for HCI">
            </div>
			<div class="col-lg-6 mb-3"><p class="annotation">[Research focus diagram inspired by professor <a href="https://people.eecs.berkeley.edu/~bjoern/">Bjoern Hartmann</a>]</p></div>
        </div> -->

        <div class="row">
            
            <!-- left column -->
            <div class="col-lg-8 mb-2">
                <h2>
                    Selected Research
                </h2>

                <div class="research-tabs">
                    <button onclick="showResearchTab('selected')">Selected</button>
                    <button onclick="showResearchTab('all')">All</button>
                </div>

                <!-- <a id="toggle-more-research" href="#">More &gt;</a> -->

                <br/>

                <div id="research-selected" class="research-list">

                    <div class="row research-project" data-sort="2025-01-16">
                        <div class="col-md-4">
                            <img src="/~yla879/research/AI-Afterlife/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                "AI Afterlife" as Digital Legacy: Perceptions, Expectations, and Concerns
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Shuai Ma, Yuling Sun, Xiaojuan Ma (CHI 2025) <br />
                                <!-- <a class="info" href="/~yla879/research/AI-Afterlife/video.mp4">[Video]</a>  -->
                                <a class="info" href="https://doi.org/10.1145/3706598.3713933">[PDF]</a>
                                <!-- <a class="info" href="/~yla879/research/AI-Afterlife/prototype.jpg">[Prototype]</a> -->
                            </p>
                            <p class="strong">
                                <i class="fas fa-medal"></i> Best Paper Honorable Mention Award
                            </p>
                            <p>
                                The rise of generative AI technology has sparked interest in using digital information to create AI-generated agents as digital legacies.
                                This paper presents a qualitative study examining users' perceptions, expectations, and concerns regarding AI-generated agents as digital legacies.
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2024-05-01">
                        <div class="col-md-4">
                            <img src="/~yla879/research/late-life migrants/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                Unpacking ICT-supported Social Connections and Support of Late-life Migration: From the Lens of Social Convoys
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Shuai Ma, Yuling Sun. (CHI 2024) <br />
                                <a class="info" href="https://www.youtube.com/watch?v=c02uMf9_7S0">[Video]</a> 
                                <a class="info" href="https://doi.org/10.1145/3613904.3642898">[DOI]</a>
                                <a class="info" href="/~yla879/research/late-life migrants/chi24-1000.pdf">[PDF]</a>
                            </p>
                            <!-- <p class="strong">
                                <i class="fas fa-medal"></i> Honorable Mention Award
                            </p> -->
                            <p>
                                Migration and aging-related dilemmas have limited the opportunities for late-life migrants to rebuild social connections and access support. 
                                <!-- While research on migrants has drawn increasing attention in HCI, limited attention has been paid to the increasing number of late-life migrants.  -->
                                This paper reports a qualitative study examining the social connections and support of late-life migrants. 
                                <!-- In particular, drawing on the social convoy model, we pay specific attention to the dynamic changes of late-life migrants’ social convoy, 
                                the supporting roles each convoy plays, the functions ICT plays in the process, as well as the encountered challenges and expectations of late-life migrants regarding ICT-supported social convoys. 
                                Based on these findings, we deeply discuss the role of the social convoy in supporting more targeted social support for late-life migrants, 
                                as well as broader migrant communities. Finally, we offer late-life migrant-oriented design considerations. -->
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2024-05-01">
                        <div class="col-md-4">
                            <img src="/~yla879/research/decision making-confidence/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                "Are You Really Sure?'' Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making
                            </h6>
                            <p class="text-muted">
                                Shuai Ma, Xinru Wang, <u>Ying Lei</u>, Chuhan Shi, Ming Yin, Xiaojuan Ma. (CHI 2024) <br />
                                <a class="info" href="https://www.youtube.com/watch?v=gCRIX8CCUKs">[Video]</a> 
                                <a class="info" href="https://doi.org/10.1145/3613904.3642671">[DOI]</a>
                                <a class="info" href="/~yla879/research/decision making-confidence/self_confidence_calibration.pdf">[PDF]</a>
                                <!-- <a class="info" href="/~yla879/research/decision making-confidence/website.jpg">[Website]</a> -->
                            </p>
                            <!-- <p class="strong">
                                <i class="fas fa-medal"></i> Honorable Mention Award
                            </p> -->
                            <p>
                                In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI. 
                                This paper approaches this problem from a human-centered perspective, "human self-confidence calibration". 
                                <!-- We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence. 
                                In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness. 
                                Then in our second study, We propose three calibration mechanisms and compare their effects on humans’ self-confidence and user experience. 
                                Subsequently, our third study investigates the effects of self-confidence calibration on AI-assisted decision-making. 
                                Results show that calibrating human self-confidence enhances human-AI team performance and encourages more rational reliance on AI (in some aspects) compared to uncalibrated baselines. 
                                Finally, we discuss our main findings and provide implications for designing future AI-assisted decision-making interfaces. -->
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2023-05-01">
                        <div class="col-md-4">
                            <img src="/~yla879/research/decision making-trust/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making
                            </h6>
                            <p class="text-muted">
                                Shuai Ma, <u>Ying Lei</u>, Xinru Wang, Chengbo Zheng, Chuhan Shi, Ming Yin, Xiaojuan Ma. (CHI 2023) <br />
                                <a class="info" href="https://www.youtube.com/watch?v=AWdUDaEqoSs">[Video]</a> 
                                <a class="info" href="https://doi.org/10.1145/3544548.3581058">[DOI]</a>
                                <a class="info" href="/~yla879/research/decision making-trust/human_AI_CL.pdf">[PDF]</a>
                                <!-- <a class="info" href="/~yla879/research/decision making-trust/website.jpg">[Website]</a> -->
                            </p>
                            <!-- <p class="strong">
                                <i class="fas fa-medal"></i> Honorable Mention Award
                            </p> -->
                            <p>
                                In AI-assisted decision-making, it is critical for human decision-makers to know when to trust AI and when to trust themselves. 
                                <!-- However, prior studies calibrated human trust only based on AI confidence indicating AI's correctness likelihood (CL) but ignored humans' CL, hindering optimal team decision-making.  -->
                                <!-- To mitigate this gap,  -->
                                In this paper, we proposed to promote humans' appropriate trust based on the CL of both sides at a task-instance level. 
                                <!-- We first modeled humans’ CL by approximating their decision-making models and computing their potential performance in similar instances. 
                                We demonstrated the feasibility and effectiveness of our model via two preliminary studies. 
                                Then, we proposed three CL exploitation strategies to calibrate users’ trust explicitly/implicitly in the AI-assisted decision-making process. 
                                Results from a between-subjects experiment (N=293) showed that our CL exploitation strategies promoted more appropriate human trust in AI, compared with only using AI confidence. 
                                We further provided practical implications for more human-compatible AI-assisted decision-making. -->
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                </div>
                <div id="research-all" class="research-list" style="display:none;">

                    <div class="row research-project" data-sort="2025-05-16">
                        <div class="col-md-4">
                            <img src="/~yla879/research/digital-calendar/cover.png" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                FamilyCanvas: A Family Whiteboard Calendar Designed for Blended Families
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Carman Neustaedter (GI 2025 Poster) <br />
                                <!-- <a class="info" href="/~yla879/research/AI-Afterlife/video.mp4">[Video]</a>  -->
                                <a class="info" href="/~yla879/research/digital-calendar/GI25Poster.pdf">[PDF]</a>
                                <!-- <a class="info" href="/~yla879/research/AI-Afterlife/prototype.jpg">[Prototype]</a> -->
                            </p>
                            <p>
                                Blended families with joint custody face challenges in scheduling and remote connections. 
                                In this study, we aim to design a digital calendar to address these challenges. 
                                At this stage, we have designed and developed an initial prototype calendar called FamilyCanvas using a whiteboard metaphor. 
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2025-01-16">
                        <div class="col-md-4">
                            <img src="/~yla879/research/AI-Afterlife/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                "AI Afterlife" as Digital Legacy: Perceptions, Expectations, and Concerns
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Shuai Ma, Yuling Sun, Xiaojuan Ma (CHI 2025) <br />
                                <!-- <a class="info" href="/~yla879/research/AI-Afterlife/video.mp4">[Video]</a>  -->
                                <a class="info" href="https://arxiv.org/pdf/2502.10924">[PDF]</a>
                                <!-- <a class="info" href="/~yla879/research/AI-Afterlife/prototype.jpg">[Prototype]</a> -->
                            </p>
                            <p class="strong">
                                <i class="fas fa-medal"></i> Best Paper Honorable Mention Award
                            </p>
                            <p>
                                The rise of generative AI technology has sparked interest in using digital information to create AI-generated agents as digital legacies.
                                This paper presents a qualitative study examining users' perceptions, expectations, and concerns regarding AI-generated agents as digital legacies.
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2024-08-01">
                        <div class="col-md-4">
                            <img src="/~yla879/research/mid-air typing/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                Understanding the Effects of Restraining Finger Coactivation in Mid-Air Typing: from a Neuromechanical Perspective
                            </h6>
                            <p class="text-muted">
                                Hechuan Zhang, Xuewei Liang, <u>Ying Lei</u>, Yanjun Chen, Zhenxuan He, Yu Zhang, Lihan Chen, Hongnan Lin, Teng Han, Feng Tian. (UIST 2024) <br />
                                <a class="info" href="https://www.youtube.com/watch?v=fZ4dgLCYmZo">[Video]</a> 
                                <a class="info" href="https://doi.org/10.1145/3654777.3676441">[DOI]</a>
                                <a class="info" href="/~yla879/research/mid-air typing/mid-air typing.pdf">[PDF]</a>
                                <a class="info" href="/~yla879/research/mid-air typing/artefact.jpg">[Artifact]</a>
                            </p>
                            <!-- <p class="strong">
                                <i class="fas fa-medal"></i> Honorable Mention Award
                            </p> -->
                            <p>
                                Typing in mid-air is often perceived as intuitive yet presents challenges due to finger coactivation, 
                                a neuromechanical phenomenon that involves involuntary finger movements stemming from the lack of physical constraints. 
                                <!-- Previous studies were used to examine and address the impacts of finger coactivation using algorithmic approaches.  -->
                                <!-- Alternatively,  -->
                                This paper explores the neuromechanical effects of finger coactivation on mid-air typing, 
                                aiming to deepen our understanding and provide valuable insights to improve these interactions. 
                                <!-- We utilized a wearable device that restrains finger coactivation as a prop to conduct two mid-air studies, 
                                including a rapid finger-tapping task and a ten-finger typing task. The results revealed that restraining coactivation not only reduced mispresses, 
                                which is a classic coactivated error always considered as harm caused by coactivation. 
                                Unexpectedly, the reduction of motor control errors and spelling errors, thinking as non-coactivated errors, 
                                also be observed. Additionally, the study evaluated the neural resources involved in motor execution using functional Near Infrared Spectroscopy (fNIRS), 
                                which tracked cortical arousal during mid-air typing. The findings demonstrated decreased activation in the primary motor cortex of the left hemisphere when coactivation was restrained, 
                                suggesting a diminished motor execution load. This reduction suggests that a portion of neural resources is conserved, 
                                which also potentially aligns with perceived lower mental workload and decreased frustration levels.	 -->
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2024-05-01">
                        <div class="col-md-4">
                            <img src="/~yla879/research/late-life migrants/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                Unpacking ICT-supported Social Connections and Support of Late-life Migration: From the Lens of Social Convoys
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Shuai Ma, Yuling Sun. (CHI 2024) <br />
                                <a class="info" href="https://www.youtube.com/watch?v=c02uMf9_7S0">[Video]</a> 
                                <a class="info" href="https://doi.org/10.1145/3613904.3642898">[DOI]</a>
                                <a class="info" href="/~yla879/research/late-life migrants/chi24-1000.pdf">[PDF]</a>
                            </p>
                            <!-- <p class="strong">
                                <i class="fas fa-medal"></i> Honorable Mention Award
                            </p> -->
                            <p>
                                Migration and aging-related dilemmas have limited the opportunities for late-life migrants to rebuild social connections and access support. 
                                <!-- While research on migrants has drawn increasing attention in HCI, limited attention has been paid to the increasing number of late-life migrants.  -->
                                This paper reports a qualitative study examining the social connections and support of late-life migrants. 
                                <!-- In particular, drawing on the social convoy model, we pay specific attention to the dynamic changes of late-life migrants’ social convoy, 
                                the supporting roles each convoy plays, the functions ICT plays in the process, as well as the encountered challenges and expectations of late-life migrants regarding ICT-supported social convoys. 
                                Based on these findings, we deeply discuss the role of the social convoy in supporting more targeted social support for late-life migrants, 
                                as well as broader migrant communities. Finally, we offer late-life migrant-oriented design considerations. -->
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2022-08-26">
                        <div class="col-md-4">
                            <img src="/~yla879/research/OPO-FCM/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                OPO-FCM: A Computational Affection based OCC-PAD-OCEAN Federation Cognitive Modeling Approach
                            </h6>
                            <p class="text-muted">
                                Feng Liu, Han-Yang Wang, Si-Yuan Shen, Xun Jia, Jing-Yi Hu, Jia-Hao Zhang, Xi-Yi Wang, <u>Ying Lei</u>, Aimin Zhou, Jiayin Qi (IEEE TCSS 2022) <br />
                                <!-- <a class="info" href="https://www.youtube.com/watch?v=fZ4dgLCYmZo">[Video]</a>  -->
                                <a class="info" href="https://doi.org/10.1109/TCSS.2022.3199119">[DOI]</a>
                                <a class="info" href="/~yla879/research/OPO-FCM/opo-fcm.pdf">[PDF]</a>
                                <!-- <a class="info" href="/~yla879/research/OPO-FCM/website.jpg">[Website]</a> -->
                            </p>
                            <!-- <p class="strong">
                                <i class="fas fa-medal"></i> Honorable Mention Award
                            </p> -->
                            <p>
                                In recent years, integrating deep learning with interpretable cognitive modeling based on emotional psychology has been a challenging issue. 
                                To address this problem, a cognitive model called OPO-FCM was developed, combining the VGG-FACS-OCC model with OCC-PAD-OCEAN frameworks based on fer2013 expression features to create a computational approach for emotion-based modeling.
                                <!-- In recent years, it is a difficult issue to integrate the deep cross-fertilization and interpretable cognitive modeling methods from the basic theory of emotional psychology with deep learning and other algorithms. 
                                To address this problem, a cognitive model that integrates the VGG-facial action coding system (FACS)-OCC model based on fer2013 expression features and the OCC-pleasure-arousal-dominance (PAD)-openness, conscientiousness, extraversion, agreeableness, and neuroticism (OCEAN) fusion of the basic theory of emotional psychology, namely, a computational affection-based OCC-PAD-OCEAN federation cognitive modeling (OPO-FCM), is constructed.  -->
                                <!-- By constructing this model and performing formal proof algorithms, it is shown that the OPO-FCM can acquire expression features in video streams, complete the acquisition of expression features in videos by training a deep neural network, map expressions to the PAD emotion space through the established expression-basic emotions-emotion space mapping relationship, and finally complete the mapping of the average emotion over a period time. 
                                The information of personality space is obtained through it. Finally, the experimental simulation of the model is conducted, 
                                and the results show that the average accuracy of the valid tested personalities is 79.56%. 
                                This article takes the knowledge-driven approach of emotional psychology as a starting point and combines deep learning techniques to construct interpretable cognitive models, 
                                thus providing new ideas for future cross-innovation between computer technology and psychology theory.	 -->
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2022-08-26">
                        <div class="col-md-4">
                            <img src="/~yla879/research/LDM-EEG/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                LDM-EEG:A Lightweight EEG Emotion Recognition Method Based on Dual-stream Structure Scaling and Multiple Attention Mechanisms
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Feng Liu (Computer Science 2023) <br />
                                <!-- <a class="info" href="https://www.youtube.com/watch?v=fZ4dgLCYmZo">[Video]</a>  -->
                                <a class="info" href="https://doi.org/10.11896/jsjkx.220300262">[DOI]</a>
                                <a class="info" href="/~yla879/research/LDM-EEG/LDM-EEG.pdf">[PDF]</a>
                            </p>
                            <!-- <p class="strong">
                                <i class="fas fa-medal"></i> Honorable Mention Award
                            </p> -->
                            <p>
                                EEG emotion recognition is a multi-channel time-series signal classification problem with high complexity, high information density and massive data. 
                                In order to achieve optimal accuracy and performance of EEG emotion recognition with fewer computational parameters while maintaining the existing classification accuracy, this paper proposes a lightweight network(LDM-EEG) based on dual-stream structural scaling and multiple attention mechanisms. 
                                <!-- The network takes the time-space and frequency-space maps constructed based on the differential entropy features of EEG signals as the input,processes the two features separately using a symmetric dual-stream structure, 
                                achieves lightweighting through a novel parameter-saving residual module and a network scaling mechanism, and enhances the model feature aggregation capability using a novel channel-time/frequency-space multiple attention mechanism and a post-attention mechanism. 
                                Experimental results show that the accuracy of the model is 95.18% with significantly reduced number of parameters,which achieves the optimal result in the domain.
                                Further,about 98% reduction in the number of parameters has been achieved with slightly lower accuracy than the existing models.	 -->
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="2022-08-26">
                        <div class="col-md-4">
                            <img src="/~yla879/project/music/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                Mind Catcher-Affective Computing based Edge-side Music Therapy System
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Jiaqi Ruan, Yihao Zhou, Shuya Xu, Ruifang Cui (National-level Undergraduate Innovation Project) <br />
                                <!-- <a class="info" href="/~yla879/project/music/video.mp4">[Video]</a>  -->
                                <a class="info" href="/~yla879/project/music/report.pdf">[PDF]</a>
                                <a class="info" href="/~yla879/project/music/application.jpg">[System]</a>
                            </p>
                            <p>
                                Identifying and regulating negative emotions is a challenging field. 
                                This project uses music therapy as a medium to influence emotions, combining music theory and computer innovation technology to 
                                create an edge-side music regulation system based on emotional computing.
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <div class="row research-project" data-sort="none">
                        <div class="col-md-4">
                            <img src="/~yla879/research/WatchGuardian/recording_interface.png" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                WatchGuardian: Enabling User-Defined Personalized Just-in-Time Intervention on Smartwatch
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Yancheng Cao, Will Wang, Yuanzhe Dong, Changchang Yin, Weidan Cao, Ping Zhang, Jingzhen Yang, Bingsheng Yao, Yifan Peng, Chunhua Weng, Randy Auerbach, Lena Mamykina, Dakuo Wang, Yuntao Wang, Xuhai Xu (Under Review) <br />
                                <a class="info" href="https://arxiv.org/pdf/2502.05783">[PDF]</a>
                            </p>
                            <p>
                                While just-in-time interventions (JITIs) have effectively targeted common health behaviors, individuals often have unique needs to intervene in personal undesirable actions that can negatively affect physical, mental, and social well-being. 
                                We present WatchGuardian, a smartwatch-based JITI system that empowers users to define custom interventions for these personal actions with a small number of samples. 
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div>

                    <!-- <div class="row research-project" data-sort="none">
                        <div class="col-md-4">
                            <img src="/~yla879/research/CGB/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                A Study on Large-Scale, Emergency Collaboration in Local Community
                            </h6>
                            <p class="text-muted">
                                <u>Anonymized</u> (Under Review) <br />
                            </p>
                            <p>
                                Examining the collaboration in the local community and then designing tailored technologies to better support that is always a topic of importance in CSCW. 
                                While existing studies have been primarily conducted within small-scale community collaboration scenarios, validated through experimental research, this study joins this research effort by contributing an empirical understanding of Community Group Buying (CGB), a large-scale, emergency-driven community collaborative behaviors in Shanghai, China. 
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div> -->

                    <!-- <div class="row research-project" data-sort="2022-08-26">
                        <div class="col-md-4">
                            <img src="/~yla879/project/handwriting/cover.jpg" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                Handwriting Recognition system
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u>, Yihong Wu  (Course Project) <br />
                                <a class="info" href="/~yla879/project/handwriting/video.mp4">[Video]</a> 
                                <a class="info" href="/~yla879/project/handwriting/report.pdf">[PDF]</a>
                                <a class="info" href="/~yla879/project/handwriting/website.png">[Prototype]</a>
                            </p>
                            <p>
                                Handwriting recognition system of both alphabet and numbers, which use both machinae learning and deep learning methods.
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div> -->

                    <!-- <div class="row research-project" data-sort="2022-08-26">
                        <div class="col-md-4">
                            <img src="/~yla879/project/deposit photo.png" class="portrait" alt="cover">
                        </div>
                        <div class="col-md-8">
                            <h6>
                                Interactive Storytelling Agents for Child Safety Education: Design, Implementation and Evaluation
                            </h6>
                            <p class="text-muted">
                                <u>Ying Lei</u> (Bachelor Thesis) <br />
                                <a class="info" href="/~yla879/project/bachelor/PDF.pdf">[PDF]</a>
                            </p>
                            <p>
                                Based on the importance of child safety education and the current challenges, 
                                this paper aims to use Human Centered Artificial Intelligence (HCAI) technology to 
                                build an interactive storytelling agent for child safety education as a means to overcome the challenges in child safety education.
                            </p>
                        </div>
                        <div class="col-md-12"> 
                            <hr class="dash">
                        </div>
                    </div> -->

                </div>
            </div>
            <!-- right column -->
            <div class="col-lg-4 mb-2">
				
                <h2>Latest News</h2>
                <ul class="news" style="font-size: 13px">
                    <li>May 2025: Attend GI 2025 @Kelowna in person.</li>
                    <li>May 2025: Attend CHI 2025 @Yokohama in person.</li>
                    <li>April 2025: Won SFU Graduate Fellowship!</li>
                    <li>Mar 2025: Won CHI Best Paper Honorable Mention! </li>
                    <li>Mar 2025: Won SFU FCAT Graduate Fellowship!</li>
                </ul>
                <ul class="news" id="news-more" style="font-size: 13px">
                    <li>Jan 2025: One paper got accepted by CHI 2025! </li>
                    <li>Sept 2024: Start a new journey at SIAT, SFU. </li>
                    <li>Jul 2024: One papers got accepted by UIST 2024! </li>
                    <li>May 2024: Attend CHI 2024 @Hawaii virtually. </li>
                    <li>Jan 2024: Two papers got accepted by CHI 2024! </li>
                    <li>Jun 2023: Pass my bachelor thesis defense. </li>
                    <l1>May 2023: Attend CHI 2023 @Hamburg virtually. </l1>
                    <li>Jan 2023: One papers got accepted by CHI 2023! </li>
                </ul>
                <a id="toggle-more-news" href="#">More &gt;</a>
				<br/>
				<br/>
				
                <h2>Press</h2>
                <ul class="press" style="font-size: 13px">
                    <li>Aug 2025: China Daily @HK report on <a href="https://doi.org/10.1145/3706598.3713933">"AI Afterlife"</a></li> 
                    <!-- with <a href="https://shuaima.cc/">Dr. Ma</a> -->
                </ul>
                <br/>
                
                <h2>Awards</h2>
                <ul class="award" style="font-size: 13px">
                    <li>2025 SFU Graduate Fellowship </li>
                    <li>2025 CHI Best Paper Honorable Mention (Top 5%)</li>
                    <li>2025 CHI Special Recognition for Outstanding Review</li>
					<li>2025 SFU FCAT Graduate Fellowship</li>
                    <li>2024 Gary Marsden Travel Awards</li>
                </ul>
                <ul class="award" id="award-more" style="font-size: 13px">
					<li>2023 Shanghai Outstanding Graduates Award</li>
                    <li>2022 Huaxin Scholarship (Top 1%)</li>
					<li>2022 National 2nd Prize in Outsourcing Innov.& Ent. Competition (Top 1%)</li>
                    <li>2022 National 2nd Prize in Computer Design Competition</li>
					<li>2021 National 3rd Prize in ICT Competition - Innovation Track</li>
                    <li>2022 Eastern Region 1st Prize in Outsourcing Innov.& Ent. Contest</li>
					<li>2022 Provincial 2nd Prize in Undergraduate Mathematical Modeling Contest</li>
                    <li>2021 National Key Project Fund (Rank: 3/667)</li>
					<li>2021 International Honorable Mention in Mathematical Modeling Contest (MCM/ICM)</li>
                </ul>
                <a id="toggle-more-award" href="#">More &gt;</a>
                <br/>
				<br/>
				
                <h2>Service</h2>
				
				<ul style="font-size: 13px">
					<li>Review: CHI '23 '25, CSCW '25, IMWUT '25, ACM Health</li>
                    <li>Special Recognition for Outstanding Review: CHI '25</li>
				</ul>

                <h2>Teaching</h2>
				
				<ul style="font-size: 13px">
					<li>Spring 2025 - IAT 167 Digital Games: Mechanics, Design and Programming (TA @SFU SIAT)</li>
				</ul>

                <!-- <h2>Academic Experience</h2>
                <ul class="news" style="font-size: 13px">
                    <li>ACSP@THU/UW, with Prof. Xuhai "Orson" Xu, intern, 2024 spring/summer (remote)</li>
                    <li>Institute of Software, Chinese Academy of Sciences, with Prof. Teng Han, intern, 2023 summer</li>
                    <li>HKUST, with Prof. Xiaojuan Ma, intern, 2022 summer (remote)</li>
					
	                </ul>
	                <ul class="news" id="news-more" style="font-size: 13px">
                    </ul>
                    <a id="toggle-more-news" href="#">More &gt;</a>
				<br/> -->
				<br/>
				
                <!-- <div class="mt-3 tweets">
                    <a class="twitter-timeline" width="100%" height="2400" href="https://twitter.com/yanghci">
                        Tweets by yanghci
                    </a>
                    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                </div> -->
				
            </div>
            <!-- /right column -->
        </div>

    </div>

<footer>
    <div class="container">
		<small> &copy; 2024 All rights reserved. Webpage template from <a href="https://www.yangzhang.dev/">Yang Zhang</a></small>
        <!-- <small> &copy; 2024 All rights reserved. </small> -->
	</div>
        
</footer>

</body>



<script src="https://cdn.jsdelivr.net/npm/jquery@3.4/dist/jquery.min.js"></script>
<script src="selected-pub.js"></script>
<script>
    $('#toggle-more-news').click(function () {
        $('#news-more').slideToggle(function() {
            $('#news-more').is(':visible') ? $('#toggle-more-news').text('< Less') : $('#toggle-more-news').text('More >');
        });
        return false;
    });

    $('#toggle-more-award').click(function () {
        $('#award-more').slideToggle(function() {
            $('#award-more').is(':visible') ? $('#toggle-more-award').text('< Less') : $('#toggle-more-award').text('More >');
        });
        return false;
    });

    $('#toggle-more-research').click(function () {
        $('#research-more').slideToggle(function() {
            if  ($('#toggle-more-research').text('< show less') ){
                $('#toggle-more-research').text('show more >');
                $('#research-selected').is(':visible');
            } else  {
                $('#toggle-more-research').text('show less more');
                $('#research-selected').is(':visible');
            }
        });
        return false;
    });

    $(window).on('scroll', function () {
        $('video').each(function () {
            var video = this;
            var rect = video.getBoundingClientRect();

            if (
                rect.top >= 0 && rect.left >= 0 &&
                rect.bottom <= $(window).height() &&
                rect.right <= $(window).width()
            ) {
                video.play();
            } else {
                video.pause();
            }
        });
    });

    var researchProjects = $('.research-projects').html();

    var researchProjectsSorted = $('div.research-project').sort(function (a, b) {
        var contentA = $(a).attr('data-sort');
        var contentB = $(b).attr('data-sort');
        return (contentA < contentB) ? 1 : (contentA > contentB) ? -1 : 0;
    });

    $('.sort-by-date').click(function () {
        $(this).toggleClass('active');
        if ($(this).hasClass('active')) {
            $('.research-projects').html(researchProjects);
        } else {
            $('.research-projects').html(researchProjectsSorted);
        }
    });
	
	$('.research-projects').html(researchProjectsSorted);

    document.querySelectorAll('.email-anchor').forEach(function(a) {
        a.href = 'mailto:' + ['ying_lei', 'sfu.ca'].join('@');
    });
</script>

</html>
